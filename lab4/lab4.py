# -*- coding: utf-8 -*-
"""lab4

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OcVptog_VsARxqI77eNHCb7eamJImF0U

ZAD 1
"""

import re

import numpy as np
import pandas as pd
import seaborn as sn
import matplotlib.pyplot as plt
from matplotlib import gridspec

import tensorflow_hub as hub
import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
!wget www.di.ens.fr/~lelarge/MNIST.tar.gz
!tar -zxvf MNIST.tar.gz

!pip install scikit-learn==1.1.3
from sklearn.datasets import load_boston, fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer
from sklearn.metrics import mean_squared_error, classification_report, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.neural_network import MLPRegressor
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR, SVC
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.utils import check_array

import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

RANDOM_SEED=68
torch.manual_seed(RANDOM_SEED)

# torch.backends.cudnn.enabled = False
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
DEVICE

def read_MNIST(bs_train: int, bs_test: int, mode: str = 'zad2a'):
    if mode == 'zad2a':
        transformation_pipeline = torchvision.transforms.Compose([
            torchvision.transforms.ToTensor(),
        ])
    elif mode == 'zad2b':
        transformation_pipeline = torchvision.transforms.Compose([
            torchvision.transforms.ToTensor(),
            torchvision.transforms.Normalize((0.1307,), (0.3081,))
        ])
    else:
        raise

    train_dataset = torchvision.datasets.MNIST('./',
                                               train=True,
                                               download=True,
                                               transform=transformation_pipeline)
    train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset,
                                                                 [50000, 10000])
    train_loader = torch.utils.data.DataLoader(train_dataset,
                                               batch_size=batch_size_train,
                                               shuffle=True,
                                               worker_init_fn=np.random.seed(RANDOM_SEED))
    valid_loader = torch.utils.data.DataLoader(valid_dataset,
                                               batch_size=batch_size_train,
                                               shuffle=True,
                                               worker_init_fn=np.random.seed(RANDOM_SEED))

    test_dataset = torchvision.datasets.MNIST('./',
                                              train=False,
                                              download=True,
                                              transform=transformation_pipeline)
    test_loader = torch.utils.data.DataLoader(test_dataset,
                                              batch_size=batch_size_test,
                                              shuffle=True,
                                               worker_init_fn=np.random.seed(RANDOM_SEED))

    if mode == 'zad2a':
        return list(train_loader)[:15], list(valid_loader)[:60], test_loader
    elif mode == 'zad2b':
        return list(train_loader)[:500], valid_loader, test_loader
    else:
        raise

class Clean20NewsGroup:
    @staticmethod
    def clean_20newsgroup(dataset: list):
        stopword_list = list(set(stopwords.words('english')))+["'m", "'ll", "'d", "'ve"]
        dataset = [Clean20NewsGroup.clean_starting_lines(post) for post in dataset]
        dataset = [Clean20NewsGroup.delete_stopwords(post, stopword_list) for post in dataset]
        dataset = [Clean20NewsGroup.clean_special_chars(post) for post in dataset]
        dataset = [Clean20NewsGroup.clean_duplicate_whitespaces(post) for post in dataset]
        dataset = [post.lower() for post in dataset]
        return dataset

    @staticmethod
    def clean_duplicate_whitespaces(post: str):
        return re.sub('[ ]+', ' ', post)

    @staticmethod
    def clean_special_chars(post: str):
        return re.sub('[^A-Za-z0-9 .,?!-]+', '', post)

    @staticmethod
    def delete_stopwords(post: str, stopword_list: list):
        word_tokens = word_tokenize(post)
        return ' '.join([w for w in word_tokens if not w in stopword_list])

    @staticmethod
    def clean_starting_lines(post: str):
        to_remove = ('From:', 'Subject:', 'Reply-To:', 'In-Reply-To:', 'Nntp-Posting-Host:', 'Organization:', 'X-Mailer:', 'In article <', 'Lines:', 'NNTP-Posting-Host:', 'Summary:', 'Article-I.D.:', 'Distribution:')
        return ' '.join([line.strip() for line in post.splitlines() if (not line.startswith(to_remove)) and (len(line)!=0)])

class VanillaNet(nn.Module):
    def __init__(self, hidden_size: int):
        super(VanillaNet, self).__init__()
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(28*28, hidden_size)
        self.fc2 = nn.Linear(hidden_size, 10)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.flatten(x)
        x = F.relu(self.fc1(x))
        x = self.softmax(self.fc2(x))
        return x

class ConvNet(nn.Module):
    def __init__(self, hidden_size: int):
        super(ConvNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.dropout2d = nn.Dropout2d()
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(320, hidden_size)
        self.fc2 = nn.Linear(hidden_size, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.dropout2d(self.conv2(x)), 2))
        x = self.flatten(x)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return x

class USEEmbeddingVectorizer(object):
    def __init__(self):
        self.embed = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")
        self.batch_size = 5

    def fit(self, X, y):
        return self

    def transform(self, X):
        X_embedded = []
        [X_embedded.extend(self.embed(btc)) for btc in self.batch(X, self.batch_size)]
        return X_embedded

    @staticmethod
    def batch(iterable, n=1):
        l = len(iterable)
        for ndx in range(0, l, n):
            yield iterable[ndx:min(ndx + n, l)]

class EarlyStopping:

    """Early stops the training if validation loss doesn't improve after a given patience."""
    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):
        """
        Args:
            patience (int): How long to wait after last time validation loss improved.
                            Default: 7
            verbose (bool): If True, prints a message for each validation loss improvement.
                            Default: False
            delta (float): Minimum change in the monitored quantity to qualify as an improvement.
                            Default: 0
            path (str): Path for the checkpoint to be saved to.
                            Default: 'checkpoint.pt'
            trace_func (function): trace print function.
                            Default: print
        """
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = np.Inf
        self.delta = delta
        self.path = path
        self.trace_func = trace_func
    def __call__(self, val_loss, model):

        score = -val_loss

        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, model)
        elif score < self.best_score + self.delta:
            self.counter += 1
            if self.verbose:
                self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model)
            self.counter = 0

    def save_checkpoint(self, val_loss, model):
        '''Saves model when validation loss decrease.'''
        if self.verbose:
            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')
        torch.save(model.state_dict(), self.path)
        self.val_loss_min = val_loss

def train_model(model: torch.nn.Module, verbose: bool = False):

    # to track the training loss as the model trains
    train_losses = []
    # to track the validation loss as the model trains
    valid_losses = []
    # to track the average training loss per epoch as the model trains
    avg_train_losses = []
    # to track the average validation loss per epoch as the model trains
    avg_valid_losses = []

    all_train_losses = []
    all_valid_losses = []

    train_error_per_epoch = []
    valid_error_per_epoch = []

    # accuracy numerator
    correct_train = 0
    acc_all_train = 0
    correct_valid = 0
    acc_all_valid = 0

    # initialize the early_stopping object
    early_stopping = EarlyStopping(patience=PATIENCE, verbose=verbose)

    for epoch in range(1, NUM_EPOCHS + 1):

        ###################
        # train the model #
        ###################
        model.train()
        for batch, (input, target) in enumerate(train_loader, 1):
            input = input.to(DEVICE)
            target = target.to(DEVICE)
            optimizer.zero_grad()
            output = model(input)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            train_losses.append(loss.item())

            _, pred = torch.max(output, 1)
            correct = np.squeeze(pred.eq(target.data.view_as(pred)))
            correct_train += correct.sum().item()
            acc_all_train += correct.shape[0]

        ######################
        # validate the model #
        ######################
        model.eval()
        for input, target in valid_loader:
            input = input.to(DEVICE)
            target = target.to(DEVICE)
            output = model(input)
            loss = criterion(output, target)
            # record validation loss
            valid_losses.append(loss.item())

            _, pred = torch.max(output, 1)
            correct = np.squeeze(pred.eq(target.data.view_as(pred)))
            correct_valid += correct.sum().item()
            acc_all_valid += correct.shape[0]

        # print training/validation statistics
        # calculate average loss over an epoch
        train_loss = np.average(train_losses)
        valid_loss = np.average(valid_losses)
        avg_train_losses.append(train_loss)
        avg_valid_losses.append(valid_loss)

        # calculate accuracy per epoch
        train_acc = correct_train/acc_all_train
        valid_acc = correct_valid/acc_all_valid

        epoch_len = len(str(NUM_EPOCHS))

        if verbose:
            print_msg = (f'[{epoch:>{epoch_len}}/{NUM_EPOCHS:>{epoch_len}}] | ' +
                        f'train_loss: {train_loss:.5f} ' +
                        f'valid_loss: {valid_loss:.5f} | ' +
                        f'train_acc: {100*train_acc:.2f}% ' +
                        f'valid_acc: {100*valid_acc:.2f}%')
            print(print_msg)

        all_train_losses.extend(train_losses)
        all_valid_losses.extend(valid_losses)

        train_error_per_epoch.append(100-100*correct_train/acc_all_train)
        valid_error_per_epoch.append(100-100*correct_valid/acc_all_valid)

        train_losses = []
        valid_losses = []
        correct_train = 0
        acc_all_train = 0
        correct_valid = 0
        acc_all_valid = 0

        early_stopping(valid_loss, model)

        if early_stopping.early_stop and verbose:
            print("Early stopping")
            break

        scheduler.step()

    # load the last checkpoint with the best model
    model.load_state_dict(torch.load('checkpoint.pt'))

    return  model, avg_train_losses, avg_valid_losses, all_train_losses, all_valid_losses, train_error_per_epoch, valid_error_per_epoch

def test_model(model: torch.nn.Module, verbose: bool = False):
    test_loss = 0.0
    class_correct = list(0. for i in range(NUM_CLASSES))
    class_total = list(0. for i in range(NUM_CLASSES))
    predictions_total = []
    targets_total = []

    model.eval()
    for input, target in test_loader:
        if len(target.data) != batch_size_test:
            break
        input = input.to(DEVICE)
        target = target.to(DEVICE)
        output = model(input)
        loss = criterion(output, target)
        test_loss += loss.item()*input.size(0)
        _, pred = torch.max(output, 1)
        correct = np.squeeze(pred.eq(target.data.view_as(pred)))
        for i in range(batch_size_test):
            label = target.data[i]
            class_correct[label] += correct[i].item()
            class_total[label] += 1
            predictions_total.append(pred[i].item())
            targets_total.append(target[i].item())

    test_loss = test_loss/len(test_loader.dataset)
    test_acc = 100. * np.sum(class_correct) / np.sum(class_total)

    if verbose:
        msg = 'Test Loss: {:.6f}\n'.format(test_loss)
        print(msg)

        for i in range(NUM_CLASSES):
            if class_total[i] > 0:
                print('Test precision of %1s: %4d%% (%2d/%2d)' % (
                    str(classes[i]), 100 * class_correct[i] / class_total[i],
                    np.sum(class_correct[i]), np.sum(class_total[i])))
            else:
                print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))

        msg = '\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (test_acc,
                                                              np.sum(class_correct),
                                                              np.sum(class_total))
        print(msg)

    return predictions_total, targets_total, 100-test_acc

class RegressScoring:
    def __init__(self, y_true, y_pred, x):
        self.y_pred = y_pred
        self.y_true = y_true
        self.x = x

    def score(self, model):
        mse = self.mean_sqr_error(self.y_true, self.y_pred)
        mape = self.mean_avg_per_error(self.y_true, self.y_pred)
        model_score = self.model_scoring(self.x, self.y_true, model)
        return mse, mape, model_score

    @staticmethod
    def mean_avg_per_error(y_true, y_pred):
        # y_true, y_pred = check_array(y_true, y_pred)
        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

    @staticmethod
    def mean_sqr_error(y_true, y_pred):
        return mean_squared_error(y_true, y_pred)

    @staticmethod
    def model_scoring(x, y_true, model):
        return model.score(x, y_true)

class ClassifyScoring:
    def __init__(self, y_true, y_pred, target_names, verbose=False):
        self.y_pred = y_pred
        self.y_true = y_true
        self.target_names = target_names
        self.verbose = verbose

    def print_report(self):
        print('='*5, 'CLASSIFICATION REPORT', '='*5)
        print(classification_report(self.y_true, self.y_pred, target_names=self.target_names))

    def conf_mat(self):
        if self.verbose==True:
            self.plot_conf_mat()
        return confusion_matrix(self.y_true, self.y_pred)

    def plot_conf_mat(self):
        fig = plt.figure(figsize=(10,10))
        cm = confusion_matrix(self.y_true, self.y_pred)
        sn.heatmap(cm, annot=True, fmt='g', cmap='Blues', square=True, xticklabels=self.target_names, yticklabels=self.target_names)
        plt.title('Confusion matrix')
        plt.xlabel('Predicted label')
        plt.ylabel('True label')
        plt.show()

    def score(self):
        if self.verbose == True:
            print('='*5, 'TESTING RESULTS', '='*5)
            display(pd.DataFrame({
                'ACC': accuracy_score(self.y_true, self.y_pred),
                'PREC': precision_score(self.y_true, self.y_pred, average='weighted'),
                'REC': recall_score(self.y_true, self.y_pred, average='weighted'),
                'F1': f1_score(self.y_true, self.y_pred, average='weighted')
            }, index=[0]))
        return [accuracy_score(self.y_true, self.y_pred),
                precision_score(self.y_true, self.y_pred, average='weighted'),
                recall_score(self.y_true, self.y_pred, average='weighted'),
                f1_score(self.y_true, self.y_pred, average='weighted')]

def plot_learning_curves(train_loss: list, valid_loss: list, all_train_losses: list, all_valid_losses: list):

    max_loss = np.max(all_train_losses + all_valid_losses)

    fig = plt.figure(num=3, figsize=(10,4))
    plt.plot(range(1,len(train_loss)+1), train_loss, label='Training Loss')
    plt.plot(range(1,len(valid_loss)+1), valid_loss, label='Validation Loss')
    minposs = valid_loss.index(min(valid_loss))+1
    plt.axvline(minposs, linestyle='--', color='r', label='Early Stopping Checkpoint')
    plt.xlabel('epochs')
    plt.ylabel('loss')
    plt.ylim(0, max_loss)
    plt.xlim(0, len(train_loss)+1)
    plt.grid(True)
    plt.title('Loss per epoch')
    plt.legend(loc='upper right')
    plt.tight_layout()

    fig, ax = plt.subplots(2,1,num=4, figsize=(10,8))
    ax[0].plot(range(1,len(all_train_losses)+1), all_train_losses, label='Training Loss')
    for idx in range(NUM_EPOCHS):
        ax[0].axvline(idx*len(all_train_losses)/(len(train_loss)+1), linestyle='--', color='lightgray')
    ax[0].set_ylabel('loss')
    ax[0].set_ylim(0, max_loss)
    ax[0].set_xlim(0, len(all_train_losses)+1)
    ax[0].grid(True)
    ax[0].set_title('Loss per batch')
    ax[0].legend(loc='upper right')

    ax[1].plot(range(1,len(all_valid_losses)+1), all_valid_losses, label='Validation Loss', color='orange')
    for idx in range(NUM_EPOCHS):
        plt.axvline(idx*len(all_valid_losses)/(len(train_loss)+1), linestyle='--', color='lightgray')
    ax[1].set_xlabel('batches')
    ax[1].set_ylabel('loss')
    ax[1].set_ylim(0, max_loss)
    ax[1].set_xlim(0, len(all_valid_losses)+1)
    ax[1].grid(True)
    ax[1].legend(loc='upper right')
    plt.tight_layout()
    plt.show()

def show_example_predictions(model: torch.nn.Module):
    dataiter = iter(test_loader)
    images, labels = next(dataiter)[:]
    images = images.to(DEVICE)
    labels = labels.to(DEVICE)

    outputs = model(images)
    _, preds = torch.max(outputs.data, 1)

    fig = plt.figure(figsize=(25, 4))
    for idx in np.arange(batch_size_test):
        ax = fig.add_subplot(2, int(batch_size_test/2), idx+1, xticks=[], yticks=[])
        ax.imshow(np.squeeze(images[idx].cpu().numpy()), cmap='gray')
        ax.set_title("{} ({})".format(str(classes[preds[idx].item()]), str(classes[labels[idx].item()])),
                    color=("green" if preds[idx]==labels[idx] else "red"))
    plt.suptitle('Example predictions on single batch from testing set')
    plt.tight_layout()
    plt.show()

# Load dataset
dataset = load_boston()

# Show dataset description
print(dataset.DESCR)

# Transform dataset into pd.DataFrame
boston_df = pd.DataFrame(dataset.data)
boston_df.columns = dataset.feature_names
boston_df['TARGET'] = pd.DataFrame(dataset.target)
boston_df.head()

print('Dataset shape: ', boston_df.shape)
boston_df.describe()

numerical_columns = list(boston_df.columns)
numerical_columns.remove('CHAS')

numerical_data = np.array(boston_df[numerical_columns])
corr_mat = np.corrcoef(numerical_data.T)

fig, ax = plt.subplots(figsize=(15,15))
cmap = sn.diverging_palette(10, 160, s=50, as_cmap=True)
sn.heatmap(corr_mat, annot=True, square=True, ax=ax, vmin=-1, vmax=1, cmap=cmap,
           yticklabels=numerical_columns, xticklabels=numerical_columns)
ax.hlines([12], *ax.get_xlim())
ax.vlines([12], *ax.get_xlim())
ax.set_xlabel('features')
ax.set_ylabel('features')

corr_mat_abs = np.abs(corr_mat)

print('Top correlations between features:')
upper_tri = corr_mat_abs[:-1,:-1][np.triu_indices_from(corr_mat[:-1,:-1], k=1)]
max_ind = [np.where(corr_mat_abs[:-1,:-1]==max)[0] for max in sorted(upper_tri,reverse=True)[:3]]
_ = [print('[', numerical_columns[index[0]], '] - [', numerical_columns[index[1]],']: ', round(corr_mat[index[0], index[1]],2)) for index in max_ind]

print('Top correlations with respect to target variable:')
max_ind = [list(corr_mat_abs[-1,:-1]).index(max) for max in sorted(corr_mat_abs[-1,:-1],reverse=True)[:3]]
_ = [print('[', numerical_columns[index], '] - [TARGET]: ', round(corr_mat[index, -1],2)) for index in max_ind]

#@title Wybrana cecha do wizualizacji rozkładu
feature_name = "TARGET" #@param ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'TARGET']

fig, ax = plt.subplots(figsize=(10,7))
sn.histplot(data=boston_df, x=feature_name, ax=ax, kde=True)
plt.grid(True)

chosen_features = ['RM', 'LSTAT', 'PTRATIO', 'NOX', 'AGE']
X = boston_df[chosen_features]
Y = boston_df['TARGET']

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = RANDOM_SEED, shuffle=True)
X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size = 0.25, random_state = RANDOM_SEED, shuffle=True)

max_depths = range(1,21)
training_set_sizes = range(10,X_train.shape[0],10)

train_results = np.zeros(shape=(len(training_set_sizes),len(max_depths)))
valid_results = np.zeros(shape=(len(training_set_sizes),len(max_depths)))
for i, depth in enumerate(max_depths):
    for j, train_size in enumerate(training_set_sizes):
        regressor = DecisionTreeRegressor(random_state=RANDOM_SEED, max_depth=depth).fit(X_train.iloc[:train_size,:], Y_train[:train_size])
        train_results[j,i] = RegressScoring.mean_sqr_error(Y_train[:train_size], regressor.predict(X_train.iloc[:train_size,:]))
        valid_results[j,i] = RegressScoring.mean_sqr_error(Y_valid, regressor.predict(X_valid))

fig = plt.figure(figsize=(15, 10))
outer = gridspec.GridSpec(2, 1, wspace=0.3, hspace=0.3)

axs = plt.Subplot(fig, outer[0])
axs.plot(max_depths, train_results[-1,:], label='train')
axs.plot(max_depths, valid_results[-1,:], label='valid')
axs.grid(True)
axs.legend()
axs.set_xlabel('Model complexity [max_depth]')
axs.set_ylabel('Mean Squared Error')
fig.add_subplot(axs)

inner = gridspec.GridSpecFromSubplotSpec(1, 5, subplot_spec=outer[1], wspace=0.1, hspace=0.1)
axs1 = plt.Subplot(fig, inner[0])
axs1.plot(training_set_sizes, train_results[:,0], label='train')
axs1.plot(training_set_sizes, valid_results[:,0], label='valid')
axs1.grid(True)
axs1.set_title('max_depth=1')
axs1.set_ylabel('MSE')
axs1.set_ylim([0,np.max(valid_results[:,0])+5])
fig.add_subplot(axs1)

axs2 = plt.Subplot(fig, inner[1])
axs2.plot(training_set_sizes, train_results[:,2], label='train')
axs2.plot(training_set_sizes, valid_results[:,2], label='valid')
axs2.grid(True)
axs2.set_title('max_depth=3')
axs2.set_ylim([0,np.max(valid_results[:,0])+5])
axs2.axes.yaxis.set_ticklabels([])
fig.add_subplot(axs2, sharey=axs1)

axs3 = plt.Subplot(fig, inner[2])
axs3.plot(training_set_sizes, train_results[:,4], label='train')
axs3.plot(training_set_sizes, valid_results[:,4], label='valid')
axs3.grid(True)
axs3.set_title('max_depth=5')
axs3.set_xlabel('training set size')
axs3.set_ylim([0,np.max(valid_results[:,0])+5])
axs3.axes.yaxis.set_ticklabels([])
fig.add_subplot(axs3, sharey=axs1)

axs4 = plt.Subplot(fig, inner[3])
axs4.plot(training_set_sizes, train_results[:,9], label='train')
axs4.plot(training_set_sizes, valid_results[:,9], label='valid')
axs4.grid(True)
axs4.set_title('max_depth=10')
axs4.set_ylim([0,np.max(valid_results[:,0])+5])
axs4.axes.yaxis.set_ticklabels([])
fig.add_subplot(axs4, sharey=axs1)

axs5 = plt.Subplot(fig, inner[4])
axs5.plot(training_set_sizes, train_results[:,19], label='train')
axs5.plot(training_set_sizes, valid_results[:,19], label='valid')
axs5.grid(True), axs5.legend()
axs5.set_title('max_depth=20')
axs5.set_ylim([0,np.max(valid_results[:,0])+5])
axs5.axes.yaxis.set_ticklabels([])
fig.add_subplot(axs5, sharey=axs1)

fig.show()

chosen_depth = 5
plt.figure(dpi=200)
regressor = DecisionTreeRegressor(random_state=RANDOM_SEED, max_depth=chosen_depth).fit(X_train, Y_train)
_ = plot_tree(regressor, feature_names=chosen_features, filled=True)
plt.show()

gammas = np.reshape(np.reshape(np.repeat(np.logspace(-5,5,11), 3, axis=0), (11,3))*[1,2,5],(33))
valid_mse = []
num_sv = []

for gamma in gammas:
    regressor = make_pipeline(StandardScaler(), SVR(gamma=gamma, C=1000, epsilon=10))
    regressor.fit(X_train, Y_train)
    valid_mse.append(RegressScoring.mean_sqr_error(Y_valid, regressor.predict(X_valid)))
    num_sv.append(regressor['svr'].n_support_[0])

fig, ax = plt.subplots(1, 3, figsize=(20,5))
ax[0].plot(gammas, valid_mse, '-o'), ax[0].grid(True), ax[0].set_xscale('log'), ax[0].set_title('MSE(g)'), ax[0].set_xlabel('gamma [g]'), ax[0].set_ylabel('validation error [MSE]')
ax[1].plot(gammas, num_sv, '-o'), ax[1].grid(True), ax[1].set_xscale('log'), ax[1].set_title('Nsv(g)'), ax[1].set_xlabel('gamma [g]'), ax[1].set_ylabel('number of support vectors [Nsv]')
ax[2].plot(num_sv, valid_mse, 'o'), ax[2].grid(True), ax[2].set_title('MSE(Nsv)'), ax[2].set_xlabel('number of support vectors [Nsv]'), ax[2].set_ylabel('validation error [MSE]')
plt.show()

"""ZAD 2"""

batch_size_train = 10
batch_size_test = 1000

train_loader, valid_loader, test_loader = read_MNIST(batch_size_train, batch_size_test, 'zad2a')
classes = tuple([str(i) for i in range(0,10)])

#@title Który zbiór danych należy zwizualizować?
which_dataset = "trenuj\u0105cy" #@param ["trenujący", "walidacyjny", "testujący"]

if which_dataset == 'trenujący':
    visualize_dataset = list(train_loader)
elif which_dataset == 'walidacyjny':
    visualize_dataset = list(valid_loader)
elif which_dataset == 'testujący':
    visualize_dataset = list(test_loader)
else:
    raise

targets = []
[targets.extend(batch[-1]) for batch in visualize_dataset]

keys, counts = np.unique(targets, return_counts=True)
plt.bar(classes, counts)
plt.grid(True)
plt.title('MNIST')
plt.ylabel('Count')
plt.xlabel('Class')
plt.show()

num_examples = 3

fig, ax = plt.subplots(num_examples, 10, figsize=(10,5))
for i in range(0, 10):
    plot = 0
    for batch in visualize_dataset:
        for idx, example in enumerate(batch[-1]):
            if example == i:
                ax[plot, i].imshow(batch[0][idx][0, :, :], cmap=plt.get_cmap('gray'))
                ax[plot, i].axis('off')
                plot += 1
            if plot == num_examples:
                break
        else:
            continue
        break
plt.subplots_adjust(bottom=0.3, top=0.7, wspace=0.025, hspace=0)
plt.suptitle('Example images from the MNIST dataset')
plt.tight_layout()
plt.show()

NUM_EPOCHS = 10
PATIENCE = 5
NUM_CLASSES = 10
learning_rate = 0.1
momentum = 0.9

hidden_sizes = [2,3,5,8,12,15,20,30]

train_errs = []
valid_errs = []
test_errs = []

for model_size in hidden_sizes:
    mean_train = []
    mean_valid = []
    mean_test = []
    for i in range(10):
        model = VanillaNet(model_size)
        model = model.to(DEVICE)
        optimizer = optim.SGD(model.parameters(),
                              lr=learning_rate,
                              momentum=momentum)
        criterion = nn.CrossEntropyLoss()
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

        model, train_loss, valid_loss, train_loss_per_batch, valid_loss_per_batch, train_errors, valid_errors = train_model(model)
        predictions_total, targets_total, test_err = test_model(model)
        mean_train.append(train_errors[-1])
        mean_valid.append(valid_errors[-1])
        mean_test.append(test_err)

    train_errs.append(np.mean(mean_train))
    valid_errs.append(np.mean(mean_valid))
    test_errs.append(np.mean(mean_test))

plt.plot(hidden_sizes, train_errs)
plt.plot(hidden_sizes, valid_errs)
plt.plot(hidden_sizes, test_errs)
plt.xlabel('Model complexity [hidden_size]')
plt.ylabel('1-Accuracy (Error)')
plt.grid(True)
plt.legend(['train', 'valid', 'test'])
plt.show()

bias = train_errs
variance = valid_errs

plt.plot(hidden_sizes, bias)
plt.plot(hidden_sizes, variance)
plt.xlabel('Model complexity [hidden_size]')
plt.ylabel('Error type')
plt.grid(True)
plt.legend(['bias', 'variance'])
plt.show()

batch_size_train = 64
batch_size_test = 64

train_loader, valid_loader, test_loader = read_MNIST(batch_size_train, batch_size_test, 'zad2b')

NUM_EPOCHS = 20
PATIENCE = 2
NUM_CLASSES = 10
learning_rate = 0.05
momentum = 0.5
hidden_size = 50

model = VanillaNet(hidden_size)
model = model.to(DEVICE)

optimizer = optim.SGD(model.parameters(),
                      lr=learning_rate,
                      momentum=momentum)
criterion = nn.CrossEntropyLoss()
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

valid_errors = [100]
while(valid_errors[-1]>10):
    model, train_loss, valid_loss, train_loss_per_batch, valid_loss_per_batch, train_errors, valid_errors = train_model(model, verbose=True)

plot_learning_curves(train_loss, valid_loss, train_loss_per_batch, valid_loss_per_batch)

plt.plot(100-np.array(train_errors))
plt.plot(100-np.array(valid_errors))
plt.xlabel('Epoch')
plt.ylabel('Accuracy [%]')
plt.grid(True)
plt.legend(['train', 'valid'])
plt.ylim([70,100])
plt.show()

predictions_total, targets_total, test_err = test_model(model, verbose=True)

scoring = ClassifyScoring(y_true=targets_total, y_pred=predictions_total, target_names=classes, verbose = True)
_ = scoring.score()
print()
scoring.print_report()
cm_vanilla = scoring.conf_mat()

show_example_predictions(model)

"""##### Convolutional Neural Network"""

model = ConvNet(hidden_size)
model = model.to(DEVICE)

optimizer = optim.SGD(model.parameters(),
                      lr=learning_rate,
                      momentum=momentum)
criterion = nn.CrossEntropyLoss()
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

valid_errors = [100]
while(valid_errors[-1]>10):
    model, train_loss, valid_loss, train_loss_per_batch, valid_loss_per_batch, train_errors, valid_errors = train_model(model, verbose=True)

plot_learning_curves(train_loss, valid_loss, train_loss_per_batch, valid_loss_per_batch)

plt.plot(100-np.array(train_errors))
plt.plot(100-np.array(valid_errors))
plt.xlabel('Epoch')
plt.ylabel('Accuracy [%]')
plt.grid(True)
plt.legend(['train', 'valid'])
plt.ylim([70,100])
plt.show()

predictions_total, targets_total, test_err = test_model(model, verbose=True)

scoring = ClassifyScoring(y_true=targets_total, y_pred=predictions_total, target_names=classes, verbose=True)
_ = scoring.score()
print()
scoring.print_report()
cm_conv = scoring.conf_mat()

show_example_predictions(model)

def get_n_params(model):
 pp=0
 for p in list(model.parameters()):
     nn=1
     for s in list(p.size()):
         nn = nn*s
     pp += nn
 return pp

diff_cm = cm_vanilla - cm_conv

fig = plt.figure(figsize=(10,10))
sn.heatmap(diff_cm, annot=True, square=True, fmt='g', cmap='RdYlGn', vmin=-20, vmax=20)
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

"""ZAD 3"""

categories = ['alt.atheism', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'soc.religion.christian']
newsgroup_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=RANDOM_SEED)
newsgroup_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=RANDOM_SEED)

#@title Który zbiór danych należy zwizualizować?
which_dataset = "testuj\u0105cy" #@param ["trenujący", "testujący"]

if which_dataset == 'trenujący':
    visualize_dataset = newsgroup_train
elif which_dataset == 'testujący':
    visualize_dataset = newsgroup_test
else:
    raise

keys, counts = np.unique(visualize_dataset.target, return_counts=True)
plt.bar(categories, counts)
plt.grid(True)
plt.xticks(range(len(categories)), categories, rotation='vertical')
plt.title('20NewsGroup')
plt.ylabel('Count')
plt.xlabel('Class')
plt.show()

print('RAW DATASET')

for category in range(len(categories)):
    for idx, target in enumerate(newsgroup_train.target):
        if target == category:
            print('='*50)
            print(categories[category])
            print('='*50)
            print(newsgroup_train.data[idx])
            break

cleaned_train = Clean20NewsGroup.clean_20newsgroup(newsgroup_train.data)
cleaned_test = Clean20NewsGroup.clean_20newsgroup(newsgroup_test.data)

print('CLEANED DATASET')

for category in range(len(categories)):
    for idx, target in enumerate(newsgroup_train.target):
        if target == category:
            print('='*50)
            print(categories[category])
            print('='*50)
            print(cleaned_train[idx])
            break

bow_clf = Pipeline([
    ('bow', CountVectorizer()),
    ('clf', MultinomialNB()),
])

bow_clf.fit(cleaned_train, newsgroup_train.target)
predictions = bow_clf.predict(cleaned_test)

scoring = ClassifyScoring(y_true=newsgroup_test.target, y_pred=predictions, target_names=categories, verbose=False)
results_bow = scoring.score()
scoring.print_report()
cm_bow = scoring.conf_mat()

tfidf_clf = Pipeline([
    ('bow', CountVectorizer()),
    ('tfidf', TfidfTransformer()),
    ('clf', MultinomialNB()),
])
tfidf_clf.fit(cleaned_train, newsgroup_train.target)
predictions = tfidf_clf.predict(cleaned_test)

scoring = ClassifyScoring(y_true=newsgroup_test.target, y_pred=predictions, target_names=categories, verbose=False)
results_tfidf = scoring.score()
scoring.print_report()
cm_tfidf = scoring.conf_mat()

use_clf = Pipeline([
    ('use', USEEmbeddingVectorizer()),
    ('clf', SVC(gamma=1, C=1000)),
])
use_clf.fit(cleaned_train, newsgroup_train.target)
predictions = use_clf.predict(cleaned_test)

scoring = ClassifyScoring(y_true=newsgroup_test.target, y_pred=predictions, target_names=categories, verbose=False)
results_use = scoring.score()
scoring.print_report()
cm_use = scoring.conf_mat()

results_df = pd.DataFrame([results_bow, results_tfidf, results_use], columns=['ACC', 'PREC', 'REC', 'F1'])
results_df['experiment'] = ['BOW', 'TFIDF', 'USE']
display(results_df[['experiment', 'ACC', 'PREC', 'REC', 'F1']])

fig, axs = plt.subplots(1,3,figsize=(10,10))
vmax = np.max([cm_bow, cm_tfidf, cm_use])

sn.heatmap(cm_bow, vmin=0, vmax=vmax, annot=True, fmt='g', cmap='Blues', square=True, xticklabels=categories, yticklabels=categories, ax=axs[0], cbar=False)
axs[0].set_title('Bag of words')
axs[0].set_xlabel('Predicted label')
axs[0].set_ylabel('True label')
sn.heatmap(cm_tfidf, vmin=0, vmax=vmax, annot=True, fmt='g', cmap='Blues', square=True, xticklabels=categories, yticklabels=categories, ax=axs[1], cbar=False)
axs[1].set_title('TF-IDF')
axs[1].set_xlabel('Predicted label')
axs[1].set_yticks([])
sn.heatmap(cm_use, vmin=0, vmax=vmax, annot=True, fmt='g', cmap='Blues', square=True, xticklabels=categories, yticklabels=categories, ax=axs[2], cbar=False)
axs[2].set_title('Universal Sentence Encoder')
axs[2].set_xlabel('Predicted label')
axs[2].set_yticks([])
plt.show()